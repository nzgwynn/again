---
title: "Matching in Cluster Randomized Trials Using the Goldilocks Approach"
author:
- name: S. Gwynn Sturdevant, Ken Kleinman
  affiliation: University of Massachusetts Amherst
output: 
    pdf_document:
      citation_package: natbib
      keep_tex: true
      fig_caption: true
      latex_engine: pdflatex
bibliography: bibliography.bib
documentclass: sagej
abstract: "Matching in cluster-randomized trials (CRTs) is important, but there is no best practice. When baseline data is available, we suggest a technique which can be used to identify the best weighting of pertinent variables to enable more balance between treatment and control groups. This technique involves computing the Mahalanobis distance between the variables, use matching to find pairs, then randomizing multiple times. For each randomiziation we compute the difference in the treatment and control arms for each variable and plot these on a parallel-coordinates plot. Investigators can compare plots to identify strengths used to find matches for advantageous randomization schema."
keywords: "cluster-randomized trials, matching"
date: "`r format(Sys.time(), '%B %d, %Y')`"
---

## Introduction
To determine the efficacy of a treatment, individually randomized trials (IRTs) with blinding are the "strongest study design available" [@gatsonis2017methods]. Unfortunately, cost and study design amongst other things mean some interventions can not be randomized on an individual level. For example, education researchers decide to determine if training elementary school teachers in a reading program will affect literacy skills in third graders. Randomizing each third grader to treatment or control would not be suitable, as this equates to randomly allocating students to a teacher; rural schools tend to be smaller with only one teacher which further complicates matters. Instead, researchers may choose to randomize teachers, schools, or counties to determine any differences between the two arms of the study. Trials where groups are randomized are called cluster randomized trials (CRTs). Three reasons for conducting a CRTs are: (i) implementation occurs at the cluster level, (ii) to avoid contamination, and (iii) to measure intervention effects among cluster members who do not receive treatment [@balzer2012match; @CRTrials2009]. CRTs are "the gold standard when allocation of identifiable groups is necessary" [@murray2004design].

One challenge in CRTs is their limited sample size. Most CRTs have less than 30 independent units to randomize, though each unit may have thousands of dependent individuals [@balzer2012match]. In IRTs, investigators expect randomization to balance confounders across each arm of the trial, but the reduced size of CRTs makes this unlikely. Grouping similar units together, then randomizing, is one solution to this. Scholars debate the sizes of these groups, in particular, matching, which involves grouping 2 units together, and stratifying, where many more than 2 units are grouped[@PMVsStrat]. This article discusses matching. 

Many authors debate matching in CRTs [@balzer2012match; @CRTrials2009; @gatsonis2017methods; @diehr1995breaking; @murray1998design; @imai2009essential;
@PMVsStrat; @donner2007merits; @klar1997merits; @donner2000design; @martin1993effect]. Murray argues that "the choice of matching or stratification [of] factors is critical to the success of the procedure" [@murray1998design]. Some agree that caution must be used when matching a small number of clusters due to the decrease in power [@donner2000design; @klar1997merits; @balzer2012match; @martin1993effect]. Breaking matches in the analysis stage addresses this [@diehr1995breaking], but perhaps only when there are a small number of large clusters [@donner2007merits].  Others argue that matching is effective in a small number of clusters as it "increases the chance of the intervention groups being well-balanced" [@donner2007merits]. Imai et al. argue that not matching, in small or large sample, is "equivalent to discarding a considerable fraction of one's data" [@imai2009essential]. However, in one trial "matching actually led to a loss in statistical efficiency" [@manun1994influence, @donner2000design]. Despite all this debate few authors discuss methodologies to support the matching process [@raab2001balance]. 

Our article is an extension of methods introduced in Chapter 4 of Methods in Comparative Effectiveness Research [@gatsonis2017methods]. We suggest a method suitable for $a$ $priori$ matching using baseline data. In section 2 we outline our method, section 3 applies it to the SWAPOUT data set, and section 4 is a brief discussion.

## Methods <!-- (#sec:Method) trying to get numbering to work not sure how yet  -->

To approach this complex topic of balancing randomization in CRTs we suggest a new approach. Our approach involves controlling the strength of the matching on variables of import, and randomizing many times to obtain a distribution of possibilities when official randomization occurs. Investigators assess these distributions to determine if possible randomizations are sufficiently balanced, if not, the strength criteria is adjusted and the process begins again. The details follow.

The initial step involves prioritizing variables $(1, 2,..., n)$ from units $(1, 2, ..., m)$ to be randomized. We have
\begin{eqnarray*}
 \overline{V_1} & = & (v_{11}, v_{12},..., v_{1n})\\
 \overline{V_2} & = & (v_{21}, v_{22},..., v_{2n})\\
 \vdots & = & \vdots\\    
 \overline{V_m} & = & (v_{m1}, v_{m2},..., v_{mn})\\
\end{eqnarray*}
where $v_{ij}$ is the $i^{th}$ variable from unit $j$: each $\overline{V_i}$ contains pertinent variables from unit $i$. From here, we compute the Mahalanobois Distance matrix, **D**, by finding $d(\overline{V_i}, \overline{V_j}) = \sum_{k=1}^n \frac{(v_{ik} - v_{jk})^2}{s_k^2}$ where $s_k^2 = \frac{1}{m} \sum_{l=1}^m(v_{lk} - \overline{v_{\cdot k}})$ and $\overline{v_{\cdot k}} = \frac{1}{n} \sum_{i = 1}^m v_{ik}$. Then we use the \texttt{nmatch} function in the \texttt{designmatch} [@doi] package in \texttt{R} [@nmatch] to find $\frac{m}{2}$ pairs if $m$ is even. If $m$ is odd, the last unit should be placed in both treatment and control group for exploratory purposes. Without loss of generality, we assume $m$ is even for the remainder of this paper and note that to include an odd $m$ either treatment or control groups will include one more set of priority variables and the addition of 1 in the denominator of $d_i$.
 
Once the matching is completed and we have pairs $(\overline{C_{11}}, \overline{C_{12}}), (\overline{C_{21}}, \overline{C_{22}}), ..., (\overline{C_{\frac{m}{2}1}}, \overline{C_{\frac{m}{2}2}}).$ KEN I'M NOT SURE HOW THIS FOLLOWS THROUGH!!!!! The first match in each pair will be randomized to either treatment or control, the second to the remainder. Next, we subset $\overline{V_1}, \overline{V_2}, ..., \overline{V_m}$ into appropriate randomization subgroups: $\overline{V_{1T}}, \overline{V_{1C}}, \overline{V_{2T}}, \overline{V_{2C}},..., \overline{V_{\frac{m}{2}T}}, \overline{V_{\frac{m}{2}C}}$ where $\overline{V_{iT}} = (v_{i1}^T, v_{i2}^T,..., v_{in}^T),$ similarly for $\overline{V}_{iC}.$ Using these we find 
\begin{eqnarray*}
 d_i = \frac{| \sum_{j = 1}^{\frac{m}{2}}v_{ij}^T - \sum_{j = 1}^{\frac{m}{2}}v_{ij}^C |}{\frac{m}{2}} 
\end{eqnarray*}
 for $i = 1, 2, ..., n.$ We randomize $R$ times and find $d_{ij}$ the average difference in the two arms for the $i^{th}$ priority variable for each of the $j = 1, 2, ..., R$ re-randomizations. To visualize we draw a parallel coordinates plot where the $i^{th}$ axis plots all $d_{ij}$ for $j = 1, 2, ..., R.$ 

If the investigators find the distribution of possible randomizations unacceptable for a priority variable $j$, we introduce $\overline{S} = (s_{1}, s_{2},..., s_{n})$, which controls the strength of matching on that variable. We have
\begin{eqnarray*}
 v_{ij}^* = \prod_{i=1}^{m} v_{ij} \times s_j
\end{eqnarray*} 
which we combine to form
\begin{eqnarray*}
 \overline{V_1^*} & = & (v_{11}^*, v_{12}^*,..., v_{1n}^*) \\
 \overline{V_2^*} & = & (v_{21}^*, v_{22}^*,..., v_{2n}^*) \\
 \vdots & = & \vdots\\    
 \overline{V_m^*} & = & (v_{m1}^*, v_{m2}^*,..., v_{mn}^*) .\\
\end{eqnarray*} 
and re-run the matching to attain stronger matches for the variable $j$. We again find $d_{ij}$ and plot them. The penalty in this process is that closer matches for variable $j$ are likely to imply reduced closeness in another variable, so compromises must be made.

## Results
To demonstrate the usefulness of this technique we present a brief summary of our randomization process using baseline data from the SWAPOUT trial (Cluster-randomized Non-inferiority Trial Comparing Mupirocin vs Iodophor for Nasal Decolonization of ICU Patients to Assess Impact on Staphylococcus aureus Clinical Cultures and All-cause Bloodstream Infection During Routine Chlorhexidine Bathing) [@SOTrial]. In this non-inferiority trial, the investigators are studying whether bathing with chlorhexidine gluconate and swabbing iodophor nasal swabs are inferior to bathing with the same and mupirocin nasal swabs. In the REDUCE trial [@huang2013targeted] mupirocin nasal swabs and bathing with chlorhexidine reduced the MRSA Staphylococcus aureus (an antibiotic resistant infection) in Hospital Corporation of America intensive care units (ICU). However, physicians are reluctant to use mupirocin, an antibiotic, so investigators are assessing "swapping"" it with iodophor. 

Primary       | Secondary         |	Tertiary   |	Quaternary | Quinary
------------- | ----------------- | --------- | ---------- | ---------
Pt Days       |	Median LOS        |	Medicaid  | DC SNF     | Onc_BMT_Trp
S auer Rate   |	Comorbidity Score |	PCR Blood | Surgery    | BMT_Trp
MRSA Rate     |			
All Blood     |			
Mup-R         |			
Hx MRSA       |			
Mup Adherence |			
CHG Adherence |	

Table: Abbreviations of variables used to randomize

A COMMENT YOU MADE WAS HERE - DATA FROM EMRS/billing records - UNSURE WHAT THIS MEANS!!!!

Prior to randomization baseline data was collected for 20 months on the 137 hospitals. With this data, investigators met to prioritize baseline variables into several categories: primary, secondary, tertiary, quaternary, quinary, and not relevant to randomization. For this trial, the investigators decided that average monthly attributable days, Staphylococcus aureus Intensive Care Unit (ICU)-attributable cultures per 1,000 days, MRSA ICU-attributable cultures per 1,000 days, all pathogen ICU-attributable bacteremia cultures per 1,000 days, regional mupirocin resistance estimate, percent of admissions with MRSA diagnosis within a year, percent of mupirocin use admission to day 5, survey Chlorhexidine Glucominate were all of primary importance. Of secondary importance were median ICU length of stay, and mean Elixhauser total score. Of tertiary importance were the percentage of ICU medicaid patients, and whether or not a facility uses polymerase chain reactions to identify MRSA in blood. Next, percent admissions to skilled nursing facility (SNF), and the percent of admissions with Center for Disease Control and Prevention surveillance surgery. Lastly, if the ICU has specialty units for oncology, bone marrow transplant, or transplant units, and if the ICU has bone marrow transplant or transplant units. More information on each variable is available in appendix 1 and their abbreviations, in the same order, can be found in table 1. To ease understanding, our initial discussion will involve the first 3 variables above: Patient days, Staphylococcus aureus rate, and MRSA rate. 

```{r IntroGraph, echo=FALSE, fig.align='center', fig.show='asis', message=FALSE, warning=FALSE, cache.lazy=TRUE}
## install.packages("designmatch")
library(designmatch)

## The first is used for its parallelplot() function
## The second to add limits to all y-axises, and helps
## us make the final plot.
library(lattice)
library(grid)

## Used to read in the data
## install.packages("readxl")
library(readxl)
library(shiny)

## Used to get the right colors in
library("RColorBrewer")
mypalette<-brewer.pal(11, "RdBu")

## Functions that are used to copy and paste the parallelplot
## into soemthing that talks to viewports.
latticeGrob <- function(p){
  grob(p=p, cl="lattice")
}

drawDetails.lattice <- function(x, recording=FALSE){
  lattice:::plot.trellis(x$p, newpage=FALSE)
} 

nmatch <- function (dist_mat, subset_weight = NULL, total_pairs = NULL, 
    mom = NULL, exact = NULL, near_exact = NULL, fine = NULL, 
    near_fine = NULL, near = NULL, far = NULL, solver = NULL) 
{
    if (is.null(mom)) {
        mom_covs = NULL
        mom_tols = NULL
        mom_targets = NULL
    }
    else {
        mom_covs = mom$covs
        mom_tols = mom$tols
        mom_targets = mom$targets
    }
    if (is.null(exact)) {
        exact_covs = NULL
    }
    else {
        exact_covs = exact$covs
    }
    if (is.null(near_exact)) {
        near_exact_covs = NULL
        near_exact_devs = NULL
    }
    else {
        near_exact_covs = near_exact$covs
        near_exact_devs = near_exact$devs
    }
    if (is.null(fine)) {
        fine_covs = NULL
    }
    else {
        fine_covs = fine$covs
    }
    if (is.null(near_fine)) {
        near_fine_covs = NULL
        near_fine_devs = NULL
    }
    else {
        near_fine_covs = near_fine$covs
        near_fine_devs = near_fine$devs
    }
    if (is.null(near)) {
        near_covs = NULL
        near_pairs = NULL
        near_groups = NULL
    }
    else {
        near_covs = near$covs
        near_pairs = near$pairs
        near_groups = near$groups
    }
    if (is.null(far)) {
        far_covs = NULL
        far_pairs = NULL
        far_groups = NULL
    }
    else {
        far_covs = far$covs
        far_pairs = far$pairs
        far_groups = far$groups
    }
    if (is.null(solver)) {
        solver = "glpk"
        t_max = 60 * 15
        approximate = 1
    }
    else {
        t_max = solver$t_max
        approximate = solver$approximate
        trace = solver$trace
        round_cplex = solver$round_cplex
        solver = solver$name
    }
    n_tot = nrow(dist_mat)
    n_dec = (n_tot * (n_tot - 1)) - sum(1:(n_tot - 1))
    if (is.null(subset_weight)) {
        subset_weight = 0
    }
    cvec = t(dist_mat)[lower.tri(dist_mat)] - (subset_weight * 
        rep(1, n_dec))
    rows_far = NULL
    cols_far = NULL
    vals_far = NULL
    rows_near = NULL
    cols_near = NULL
    vals_near = NULL
    rows_mom = NULL
    cols_mom = NULL
    vals_mom = NULL
    rows_exact = NULL
    cols_exact = NULL
    vals_exact = NULL
    rows_near_exact = NULL
    cols_near_exact = NULL
    vals_near_exact = NULL
    rows_fine = NULL
    cols_fine = NULL
    vals_fine = NULL
    rows_near_fine = NULL
    cols_near_fine = NULL
    vals_near_fine = NULL
    rows_n = NULL
    cols_n = NULL
    vals_n = NULL
    rows_target = NULL
    cols_target = NULL
    vals_target = NULL
    rows_nbm = sort(rep(1:n_tot, n_tot - 1))
    temp = matrix(0, nrow = n_tot, ncol = n_tot)
    temp[lower.tri(temp)] = 1:n_dec
    temp = temp + t(temp)
    diag(temp) = NA
    cols_nbm = as.vector(t(temp))
    cols_nbm = cols_nbm[!is.na(cols_nbm)]
    vals_nbm = rep(1, (n_tot - 1) * n_tot)
    row_count = max(rows_nbm)
    rows_ind_far_pairs = list()
    if (!is.null(far_covs)) {
        rows_far = NULL
        cols_far = NULL
        vals_far = NULL
        n_far_covs = ncol(far_covs)
        for (j in 1:n_far_covs) {
            far_cov = far_covs[, j]
            if (!is.null(far_groups)) {
                far_group = far_groups[j]
                row_ind_far_all = rep(row_count + 1, n_dec)
                col_ind_far_all = rep(1:n_dec, 1)
                i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
                aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, 
                  byrow = F)
                j_ind = aux[lower.tri(aux)]
                vals_far_all = far_cov[i_ind] - far_cov[j_ind] - 
                  (far_group * rep(1, n_dec))
                row_count = max(row_ind_far_all)
            }
            if (!is.null(far_pairs)) {
                far_pair = far_pairs[j]
                aux = abs(outer(far_cov, far_cov, FUN = "-"))
                temp = as.vector(matrix(t(aux)[lower.tri(aux)], 
                  nrow = 1, byrow = TRUE))
                cols_ind_far_pairs = which(temp < far_pair)
                if (length(cols_ind_far_pairs) > 0) {
                  rows_ind_far_pairs[[j]] = row_count + (1:length(cols_ind_far_pairs))
                  vals_far_pairs = rep(1, length(cols_ind_far_pairs))
                  row_count = max(rows_ind_far_pairs[[j]])
                }
                if (length(cols_ind_far_pairs) == 0) {
                  cols_ind_far_pairs = NULL
                  rows_ind_far_pairs[[j]] = -1
                  vals_far_pairs = NULL
                }
            }
            if (!is.null(far_groups) && is.null(far_pairs)) {
                rows_far = c(rows_far, row_ind_far_all)
                cols_far = c(cols_far, col_ind_far_all)
                vals_far = c(vals_far, vals_far_all)
            }
            if (is.null(far_groups) && !is.null(far_pairs) && 
                rows_ind_far_pairs[[j]] != -1) {
                rows_far = c(rows_far, rows_ind_far_pairs[[j]])
                cols_far = c(cols_far, cols_ind_far_pairs)
                vals_far = c(vals_far, vals_far_pairs)
            }
            if (!is.null(far_groups) && !is.null(far_pairs) && 
                rows_ind_far_pairs[[j]] != -1) {
                rows_far = c(rows_far, row_ind_far_all, rows_ind_far_pairs[[j]])
                cols_far = c(cols_far, col_ind_far_all, cols_ind_far_pairs)
                vals_far = c(vals_far, vals_far_all, vals_far_pairs)
            }
            if (!is.null(far_groups) && !is.null(far_pairs) && 
                rows_ind_far_pairs[[j]] == -1) {
                rows_far = c(rows_far, row_ind_far_all)
                cols_far = c(cols_far, col_ind_far_all)
                vals_far = c(vals_far, vals_far_all)
            }
        }
    }
    rows_ind_near_pairs = list()
    if (!is.null(near_covs)) {
        rows_near = NULL
        cols_near = NULL
        vals_near = NULL
        n_near_covs = ncol(near_covs)
        for (j in 1:n_near_covs) {
            near_cov = near_covs[, j]
            if (!is.null(near_groups)) {
                near_group = near_groups[j]
                row_ind_near_all = rep(row_count + 1, n_dec)
                col_ind_near_all = rep(1:n_dec, 1)
                i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
                aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, 
                  byrow = F)
                j_ind = aux[lower.tri(aux)]
                vals_near_all = near_cov[i_ind] - near_cov[j_ind] - 
                  (near_group * rep(1, n_dec))
                row_count = max(row_ind_near_all)
            }
            if (!is.null(near_pairs)) {
                near_pair = near_pairs[j]
                aux = abs(outer(near_cov, near_cov, FUN = "-"))
                temp = as.vector(matrix(t(aux)[lower.tri(aux)], 
                  nrow = 1, byrow = TRUE))
                cols_ind_near_pairs = which(temp > near_pair)
                if (length(cols_ind_near_pairs) > 0) {
                  rows_ind_near_pairs[[j]] = row_count + (1:length(cols_ind_near_pairs))
                  vals_near_pairs = rep(1, length(cols_ind_near_pairs))
                  row_count = max(rows_ind_near_pairs[[j]])
                }
                if (length(cols_ind_near_pairs) == 0) {
                  cols_ind_near_pairs = NULL
                  rows_ind_near_pairs[[j]] = -1
                  vals_near_pairs = NULL
                }
            }
            if (!is.null(near_groups) && is.null(near_pairs)) {
                rows_near = c(rows_near, row_ind_near_all)
                cols_near = c(cols_near, col_ind_near_all)
                vals_near = c(vals_near, vals_near_all)
            }
            if (is.null(near_groups) && !is.null(near_pairs) && 
                rows_ind_near_pairs[[j]] != -1) {
                rows_near = c(rows_near, rows_ind_near_pairs[[j]])
                cols_near = c(cols_near, cols_ind_near_pairs)
                vals_near = c(vals_near, vals_near_pairs)
            }
            if (!is.null(near_groups) && !is.null(near_pairs) && 
                rows_ind_near_pairs[[j]] != -1) {
                rows_near = c(rows_near, row_ind_near_all, rows_ind_near_pairs[[j]])
                cols_near = c(cols_near, col_ind_near_all, cols_ind_near_pairs)
                vals_near = c(vals_near, vals_near_all, vals_near_pairs)
            }
            if (!is.null(near_groups) && !is.null(near_pairs) && 
                rows_ind_near_pairs[[j]] == -1) {
                rows_near = c(rows_near, row_ind_near_all)
                cols_near = c(cols_near, col_ind_near_all)
                vals_near = c(vals_near, vals_near_all)
            }
        }
    }
    if (!is.null(mom_covs) & is.null(mom_targets)) {
        rows_mom_1 = NA
        cols_mom_1 = NA
        vals_mom_1 = NA
        rows_mom_2 = NA
        cols_mom_2 = NA
        vals_mom_2 = NA
        n_covs_m = ncol(mom_covs)
        for (i in 1:n_covs_m) {
            cov_m = mom_covs[, i]
            rows_mom_1 = c(rows_mom_1, rep(row_count + i, n_dec))
            cols_mom_1 = c(cols_mom_1, 1:n_dec)
            i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
            aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, byrow = F)
            j_ind = aux[lower.tri(aux)]
            vals_mom_1 = c(vals_mom_1, cov_m[i_ind] - cov_m[j_ind] - 
                (mom_tols[i] * rep(1, n_dec)))
        }
        rows_mom_1 = rows_mom_1[-1]
        cols_mom_1 = cols_mom_1[-1]
        vals_mom_1 = vals_mom_1[-1]
        row_count = max(rows_mom_1)
        for (i in 1:n_covs_m) {
            cov_m = mom_covs[, i]
            rows_mom_2 = c(rows_mom_2, rep(row_count + i, n_dec))
            cols_mom_2 = c(cols_mom_2, 1:n_dec)
            i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
            aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, byrow = F)
            j_ind = aux[lower.tri(aux)]
            vals_mom_2 = c(vals_mom_2, cov_m[j_ind] - cov_m[i_ind] - 
                (mom_tols[i] * rep(1, n_dec)))
        }
        rows_mom_2 = rows_mom_2[-1]
        cols_mom_2 = cols_mom_2[-1]
        vals_mom_2 = vals_mom_2[-1]
        rows_mom = c(rows_mom_1, rows_mom_2)
        cols_mom = c(cols_mom_1, cols_mom_2)
        vals_mom = c(vals_mom_1, vals_mom_2)
        row_count = max(rows_mom)
    }
    if (!is.null(mom_covs) & !is.null(mom_targets)) {
        n_covs_m = ncol(mom_covs)
        rows_target = sort(rep(1:(4 * n_covs_m) + row_count, 
            n_dec))
        for (i in 1:n_covs_m) {
            cov_m = mom_covs[, i]
            cols_target = c(cols_target, rep(1:n_dec, 4))
            i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
            aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, byrow = F)
            j_ind = aux[lower.tri(aux)]
            vals_target = c(vals_target, cov_m[i_ind] - (mom_targets[i] + 
                mom_tols[i]), -1 * cov_m[i_ind] + (mom_targets[i] - 
                mom_tols[i]), cov_m[j_ind] - (mom_targets[i] + 
                mom_tols[i]), -1 * cov_m[j_ind] + (mom_targets[i] - 
                mom_tols[i]))
        }
        row_count = max(rows_target)
    }
    rows_exact = numeric()
    cols_exact = numeric()
    vals_exact = numeric()
    if (!is.null(exact_covs)) {
        n_exact_cats = ncol(exact_covs)
        for (i in 1:n_exact_cats) {
            rows_exact = c(rows_exact, rep(row_count + i, n_dec))
            cols_exact = c(cols_exact, 1:n_dec)
            dist_exact_cov = abs(outer(exact_covs[, i], exact_covs[, 
                i], "-"))
            vals_exact = c(vals_exact, dist_exact_cov[lower.tri(dist_exact_cov)])
        }
        row_count = max(rows_exact)
    }
    rows_near_exact = numeric()
    cols_near_exact = numeric()
    vals_near_exact = numeric()
    if (!is.null(near_exact_covs)) {
        n_near_exact_cats = ncol(near_exact_covs)
        for (i in 1:n_near_exact_cats) {
            rows_near_exact = c(rows_near_exact, rep(row_count + 
                j, n_dec))
            cols_near_exact = c(cols_near_exact, 1:n_dec)
            dist_near_exact_cov = abs(outer(near_exact_covs[, 
                i], near_exact_covs[, i], "-"))
            vals_near_exact = c(vals_near_exact, dist_near_exact_cov[lower.tri(dist_near_exact_cov)])
        }
        row_count = max(rows_near_exact)
    }
    if (!is.null(fine_covs)) {
        fine_covs_2 = rep(NA, nrow(fine_covs))
        n_fine_covs = ncol(fine_covs)
        j = 1
        for (i in 1:n_fine_covs) {
            aux = factor(fine_covs[, i])
            fine_covs_2 = cbind(fine_covs_2, diag(nlevels(aux))[aux, 
                ])
            if (j == 1) {
                fine_covs_2 = fine_covs_2[, -1]
            }
            j = j + 1
        }
        n_fine_cats = ncol(fine_covs_2)
        j = 1
        for (i in 1:n_fine_cats) {
            rows_fine = c(rows_fine, rep(row_count + j, n_dec))
            cols_fine = c(cols_fine, 1:n_dec)
            dist_fine_cov = outer(fine_covs_2[, i], fine_covs_2[, 
                i], "-")
            dist_fine_cov = t(dist_fine_cov)
            vals_fine = c(vals_fine, dist_fine_cov[lower.tri(dist_fine_cov)])
            if (j == 1) {
                rows_fine = rows_fine[-1]
                cols_fine = cols_fine[-1]
                vals_fine = vals_fine[-1]
            }
            j = j + 1
        }
        row_count = max(rows_fine)
    }
    if (!is.null(near_fine_covs)) {
        near_fine_covs_2 = rep(NA, nrow(near_fine_covs))
        n_near_fine_covs = ncol(near_fine_covs)
        j = 1
        for (i in 1:n_near_fine_covs) {
            aux = factor(near_fine_covs[, i])
            near_fine_covs_2 = cbind(near_fine_covs_2, diag(nlevels(aux))[aux, 
                ])
            if (j == 1) {
                near_fine_covs_2 = near_fine_covs_2[, -1]
            }
            j = j + 1
        }
        n_near_fine_cats = ncol(near_fine_covs_2)
        j = 1
        for (i in 1:n_near_fine_cats) {
            for (h in 1:2) {
                rows_near_fine = c(rows_near_fine, rep(row_count + 
                  j, n_dec))
                cols_near_fine = c(cols_near_fine, 1:n_dec)
                dist_near_fine_cov = outer(near_fine_covs_2[, 
                  i], near_fine_covs_2[, i], "-")
                dist_near_fine_cov = t(dist_near_fine_cov)
                vals_near_fine = c(vals_near_fine, dist_near_fine_cov[lower.tri(dist_near_fine_cov)])
                if (j == 1) {
                  rows_near_fine = rows_near_fine[-1]
                  cols_near_fine = cols_near_fine[-1]
                  vals_near_fine = vals_near_fine[-1]
                }
                j = j + 1
            }
        }
        row_count = max(rows_near_fine)
    }
    if (!is.null(total_pairs)) {
        rows_n = rep(row_count + 1, n_dec)
        cols_n = 1:n_dec
        vals_n = rep(1, n_dec)
        row_count = max(rows_n)
    }
    rows = c(rows_nbm, rows_far, rows_near, rows_mom, rows_target, 
        rows_exact, rows_near_exact, rows_fine, rows_near_fine, 
        rows_n)
    cols = c(cols_nbm, cols_far, cols_near, cols_mom, cols_target, 
        cols_exact, cols_near_exact, cols_fine, cols_near_fine, 
        cols_n)
    vals = c(vals_nbm, vals_far, vals_near, vals_mom, vals_target, 
        vals_exact, vals_near_exact, vals_fine, vals_near_fine, 
        vals_n)
    aux = cbind(rows, cols, vals)[order(cols), ]
    cnstrn_mat = simple_triplet_matrix(i = aux[, 1], j = aux[, 
        2], v = aux[, 3])
    Amat = cnstrn_mat
    bvec = rep(1, length(table(rows_nbm)))
    if (!is.null(far_covs)) {
        n_far_covs = ncol(far_covs)
        for (j in 1:n_far_covs) {
            if (!is.null(far_groups)) {
                bvec = c(bvec, rep(0, 1))
            }
            if (!is.null(far_pairs) && rows_ind_far_pairs[[j]] != 
                -1) {
                bvec = c(bvec, rep(0, length(table(rows_ind_far_pairs[[j]]))))
            }
        }
    }
    if (!is.null(near_covs)) {
        n_near_covs = ncol(near_covs)
        for (j in 1:n_near_covs) {
            if (!is.null(near_groups)) {
                bvec = c(bvec, rep(0, 1))
            }
            if (!is.null(near_pairs) && rows_ind_near_pairs[[j]] != 
                -1) {
                bvec = c(bvec, rep(0, length(table(rows_ind_near_pairs[[j]]))))
            }
        }
    }
    bvec = c(bvec, rep(0, length(table(rows_mom))))
    bvec = c(bvec, rep(0, length(table(rows_target))))
    if (!is.null(exact_covs)) {
        bvec = c(bvec, rep(0, ncol(exact_covs)))
    }
    if (!is.null(near_exact_covs)) {
        bvec = c(bvec, near_exact_devs)
    }
    bvec = c(bvec, rep(0, length(table(rows_fine))))
    if (!is.null(near_fine_covs)) {
        bvec_8_aux = rep(NA, length(rows_near_fine))
        bvec_8_aux[seq(1, length(rows_near_fine), 2)] = -near_fine_devs
        bvec_8_aux[seq(2, length(rows_near_fine), 2)] = near_fine_devs
        bvec = c(bvec, bvec_8_aux)
    }
    if (!is.null(total_pairs)) {
        bvec = c(bvec, total_pairs)
    }
    ub = rep(1, n_dec)
    sense = rep("L", length(table(rows_nbm)))
    if (!is.null(far_covs)) {
        n_far_covs = ncol(far_covs)
        for (j in 1:n_far_covs) {
            if (!is.null(far_groups)) {
                sense = c(sense, rep("G", 1))
            }
            if (!is.null(far_pairs) && rows_ind_far_pairs[[j]] != 
                -1) {
                sense = c(sense, rep("E", length(table(rows_ind_far_pairs[[j]]))))
            }
        }
    }
    if (!is.null(near_covs)) {
        n_near_covs = ncol(near_covs)
        for (j in 1:n_near_covs) {
            if (!is.null(near_groups)) {
                sense = c(sense, rep("L", 1))
            }
            if (!is.null(near_pairs) && rows_ind_near_pairs[[j]] != 
                -1) {
                sense = c(sense, rep("E", length(table(rows_ind_near_pairs[[j]]))))
            }
        }
    }
    sense = c(sense, rep("L", length(table(rows_mom))))
    sense = c(sense, rep("L", length(table(rows_target))))
    if (!is.null(exact_covs)) {
        sense = c(sense, rep("E", ncol(exact_covs)))
    }
    if (!is.null(near_exact_covs)) {
        sense = c(sense, rep("L", ncol(near_exact_covs)))
    }
    sense = c(sense, rep("E", length(table(rows_fine))))
    sense = c(sense, rep(c("G", "L"), length(table(rows_near_fine))/2))
    sense = c(sense, rep("E", length(total_pairs)))
    if (approximate == 1) {
        var_type = rep("C", n_dec)
    }
    else {
        var_type = rep("B", n_dec)
    }
    if (solver == "glpk") {
        dir = rep(NA, length(sense))
        dir[sense == "E"] = "=="
        dir[sense == "L"] = "<="
        dir[sense == "G"] = ">="
        bound = list(lower = list(ind = c(1:length(ub)), val = rep(0, 
            length(ub))), upper = list(ind = c(1:length(ub)), 
            val = ub))
        ptm = proc.time()
        out = Rglpk_solve_LP(cvec, Amat, dir, bvec, bounds = bound, 
            types = var_type, max = FALSE)
        time = (proc.time() - ptm)[3]
        if (out$status != 0) {
            cat(format("  Error: problem infeasible!"), "\n")
            obj_val = NA
            obj_dist_mat = NA
            id_1 = NA
            id_2 = NA
            group_id = NA
            time = NA
        }
        if (out$status == 0) {
            if (approximate == 1) {
                rel = .relaxation_n(n_tot, out$solution, dist_mat, 
                  subset_weight, "glpk", round_cplex, trace)
                out$solution = rel$sol
                out$optimum = rel$obj
                time = time + rel$time
            }
            i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
            aux = matrix(1:n_tot, nrow = n_tot, ncol = n_tot)
            j_ind = aux[lower.tri(aux)]
            group_1 = i_ind[out$solution == 1]
            group_2 = j_ind[out$solution == 1]
            max_groups = apply(cbind(group_1, group_2), 1, max)
            id_1 = group_1[max_groups <= n_tot]
            id_2 = group_2[max_groups <= n_tot]
            group_id_1 = 1:(length(id_1))
            group_id_2 = 1:(length(id_2))
            group_id = c(group_id_1, group_id_2)
            obj_val = out$optimum
            obj_dist_mat = sum(t(dist_mat)[lower.tri(dist_mat)] * 
                out$solution)
        }
    }
    return = list(obj_total = obj_val, obj_dist_mat = obj_dist_mat, 
        id_1 = id_1, id_2 = id_2, group_id = group_id, time = time)
}

makePlot = function(V, P, D, Tm, NVs, cols){
  ## Including means of columns and indicator that tells
  ## whether or not it is to be included in plot as mean (light blue)
  ## or regular data point
  CM = colMeans(P[[3]])
  G = c(rep("A", Tm), "B")
  P[[3]] = cbind(rbind(P[[3]], CM), G)
  
  ## Setting the upper values for the parallelplot
  upper <- as.numeric(P[[2]][,"Maxs"])
  
  ## Naming the columns
  CNs <- c(P[[2]][,'L'], "G")
  colnames(P[[3]]) = CNs
  
  ## add draw = FALSE, to get rid of x axis labels for hists
  p1 = parallelplot(~P[[3]][cols], P[[3]], groups = G, horizontal.axis = FALSE,
                    scales = list(x = list(rot = 90, cex = 0.5),  ## draw = FALSE,
                                  y = list(draw = FALSE)),
                    col = c("grey50", "black"), lwd = c(1,3),
                    lower = 0, upper = upper,
                    par.settings = list(axis.line = list(col = 0)),
                    panel = function(...) {
                      panel.parallel(...)
                      ## If plotting with histograms underneath take out
                      ## first grid.text
                      grid.text(0,
                                 x=unit(1:length(cols), "native"),
                                 y=unit(2, "mm"), just="bottom",
                                 gp=gpar(col="grey", cex=.7))
                      grid.text(upper[cols],
                                x=unit(c(1.01, 2, 2.97), "native"),
                                y=unit(1, "npc") - unit(2, "mm"), 
                                just="top",
                                gp=gpar(col="grey", cex=.7))
                      
                    })
  
  A  = list()
  A[[2]] = latticeGrob(p1)
  
  A 
}

# Function that does the randomizations
# M is the number of randomizations
# vars is a list of parameters which includes
# D is the data set from which the columns to be matched on are drawn
# names is the file name
# S is the optimisation method

make.Ks = function(M, vars, D, Plot, S, ToC){
  r.I = length(vars)
  No.cols = length(vars[[1]])
  I = matrix(NA, nrow = r.I, ncol = No.cols)
  for(i in 1:r.I) I[i,] = unlist(vars[[i]])
  
  dimnames(I) = list(rep(NULL, r.I), c("cols", "w", "L", "Mins", "Maxs"))
  
  ## No. of participant hospitals
  N = dim(D)[1]
  
  mymat = sdmat = matrix(NA, nrow = N, ncol = r.I)
  
  ## Combining columns
  for(i in 1:r.I) mymat[,i] = D[,I[i,"cols"]]
  
  ## Standardising and adding weights
  col.means = colMeans(mymat)
  col.sds = apply(mymat, 2, sd)
  w = as.numeric(I[,"w"])
  for(i in 1:r.I) sdmat[,i] = 
    w[i]*(mymat[,i] - col.means[i])/col.sds[i]
  
  ## Making the distance matrix
  dist_mat_obj = dist(sdmat, diag = TRUE, upper = TRUE)
  dist_mat = as.matrix(dist_mat_obj)
  
  ## Telling the computer wich method to use to solve
  t_max = 60*5
  solver = S
  approximate = 0
  solver = list(name = solver, t_max = t_max, approximate = approximate, 
                round_cplex = 0, trace_cplex = 0)
  
  ## Solving
  out = nmatch(dist_mat = dist_mat, total_pairs = floor(N/2), 
               solver = solver, subset_weight = NULL)
  
  # These guys have the row numbers of the matched pairs
  id_1 = out$id_1  
  id_2 = out$id_2 
  
  ## If there are any leftovers they get assigned where
  ## the user wants them assigned
  X = dim(D)[1] %% 2 == 0
  if(X == FALSE) {LO = sum(1:dim(D)[1]) - sum(id_1, id_2)}
  
  ## Will be used in the loop below - a place to put Ks, 
  ## filled below, then graphed after using parallel coordinate plot
  Ks = matrix(NA, nrow = M, ncol = r.I)
  
  ## Used to run the loop below
  M.seq = seq_along(1:M)
  
  for(i in M.seq){
    ## Randomising once - 0 ctl, 1 trt, (subtract one because when 
    ## making Trt below the middle step has to identify the length of 
    ## Trt or it gets fussy and won't add on the last value)
    R = replicate(length(id_1), rbinom(1, size = 1, prob = 0.5))
    S = 1 - R
    
    ## The 1 is for the leftover row that goes to the trt arm
    Trt = c(R, S)
    
    ## Making the data, TA is included and goes to the trt arm
    if (X == TRUE) {
      Dt = data.frame(mymat[c(id_1, id_2),], Trt)
    } else {
      Dt = data.frame(mymat[c(id_1, id_2, LO),], c(Trt, (ToC - 1)))
    }
    
    ## Picking out trt and ctl covariates
    Trt.CV = Dt[which(Trt == 1), 1:r.I]
    Ctl.CV = Dt[-which(Trt == 1), 1:r.I]
    
    Ks[i,] = abs((apply(Trt.CV, 2, sum) - 
                    apply(Ctl.CV, 2, sum))/N)
  }## ending for i in M.seq
  
  ## So the labels in parcoord come out nicely
  ## NEW colnames here
  colnames(Ks) = I[,'L']
  
  P = list()
  P[[1]] = cbind(id_1, id_2)
  P[[2]] = I
  P[[3]] = as.data.frame(Ks)
  P[[4]] = Trt.CV
  P[[5]] = Ctl.CV
  
  P
} ## closing the function make.Ks

D = read_excel("ArtData.xlsx",  sheet=2)[1:137,]
D = as.data.frame(D)

## Final weighting for randomization
V = list(list("Attrib_Days_Month", 1, "Pt Days", 0, 80),
         list("Outcome_Saureus", 4, "S auer Rate", 0, 0.15),
         list("Outcome_MRSA", 2, "MRSA Rate", 0, 0.15),
         list("All_Blood", 4, "All Blood", 0, 0.15),
         list("Mup-R", 2, "Mup-R", 0, 0.02),
         list("Hx_MRSA", 1,"Hx MRSA", 0, 0.01),
         list("Mup_Adherence", 1, "Mup Adherence", 0, 0.05),
         list("CHG_Adherence", 1,"CHG Adherence", 0, 0.05),
         list("Median_LOS", 3,"Median LOS", 0, 0.05),
         list("Elix_Score", 1,"Comorbidity Score", 0, 0.15),
         list("Medicaid", 0,"Medicaid", 0, 0.02),
         list("PCR_Blood", 0,"PCR_Blood", 0, 0.1),
         list("DC_SNF", 0,"DC SNF", 0, 0.02),
         list("Surgery", 1,"Surgery", 0, 0.02),
         list("Onc_BMT_Trp", 2,"Onc_BMT_Trp", 0, 0.1),
         list("BMT_Trp", 0,"BMT_Trp", 0, 0.1)
         )

## First 3 weights zeroed
V1 = list(list("Attrib_Days_Month", 0, "Pt Days", 0, 80),
         list("Outcome_Saureus", 0, "S auer Rate", 0, 0.15),
         list("Outcome_MRSA", 0, "MRSA Rate", 0, 0.15),
         list("All_Blood", 4, "All Blood", 0, 0.15),
         list("Mup-R", 2, "Mup-R", 0, 0.02),
         list("Hx_MRSA", 1,"Hx MRSA", 0, 0.01),
         list("Mup_Adherence", 1, "Mup Adherence", 0, 0.05),
         list("CHG_Adherence", 1,"CHG Adherence", 0, 0.05),
         list("Median_LOS", 3,"Median LOS", 0, 0.05),
         list("Elix_Score", 1,"Comorbidity Score", 0, 0.15),
         list("Medicaid", 0,"Medicaid", 0, 0.02),
         list("PCR_Blood", 0,"PCR_Blood", 0, 0.1),
         list("DC_SNF", 0,"DC SNF", 0, 0.02),
         list("Surgery", 1,"Surgery", 0, 0.02),
         list("Onc_BMT_Trp", 2,"Onc_BMT_Trp", 0, 0.1),
         list("BMT_Trp", 0,"BMT_Trp", 0, 0.1)
         )

## Primary outcome weighted highly others zeroed
V2 = list(list("Attrib_Days_Month", 0, "Pt Days", 0, 80),
         list("Outcome_Saureus", 8, "S auer Rate", 0, 0.15),
         list("Outcome_MRSA", 0, "MRSA Rate", 0, 0.15),
         list("All_Blood", 4, "All Blood", 0, 0.15),
         list("Mup-R", 2, "Mup-R", 0, 0.02),
         list("Hx_MRSA", 1,"Hx MRSA", 0, 0.01),
         list("Mup_Adherence", 1, "Mup Adherence", 0, 0.05),
         list("CHG_Adherence", 1,"CHG Adherence", 0, 0.05),
         list("Median_LOS", 3,"Median LOS", 0, 0.05),
         list("Elix_Score", 1,"Comorbidity Score", 0, 0.15),
         list("Medicaid", 0,"Medicaid", 0, 0.02),
         list("PCR_Blood", 0,"PCR_Blood", 0, 0.1),
         list("DC_SNF", 0,"DC SNF", 0, 0.02),
         list("Surgery", 1,"Surgery", 0, 0.02),
         list("Onc_BMT_Trp", 2,"Onc_BMT_Trp", 0, 0.1),
         list("BMT_Trp", 0,"BMT_Trp", 0, 0.1)
         )

## Making data 
## The leftover is sent to the treatment arm (ToC = 2)
P = make.Ks(M = 300, D = D, Plot = "noscreen", vars = V, S = "glpk", ToC = 2)
P1 = make.Ks(M = 300, D = D, Plot = "noscreen", vars = V1, S = "glpk", ToC = 2)
P2 = make.Ks(M = 300, D = D, Plot = "noscreen", vars = V2, S = "glpk", ToC = 2)

## Making the things to plot - cols 1, 5 only
## Half the screen on bottom
A = makePlot(V = V, P = P, D = D, Tm = 300, NVs = 2, cols = 1:3)

## Top Left
A1 = makePlot(V = V1, P = P1, D = D, Tm = 300, NVs = 2, cols = 1:3)

## Top Right
A2 = makePlot(V = V2, P = P2, D = D, Tm = 300, NVs = 2, cols = 1:3)

## The bottom one
grid.newpage()

pushViewport(viewport(x=0, y=0, width=1, height=.5,
                      just=c("left", "bottom")))
drawDetails.lattice(A[[2]])
upViewport()

## Top Left
pushViewport(viewport(x=0.01, y=0.40, width=.49, height=.6,
                      just=c("left", "bottom")))
drawDetails.lattice(A1[[2]])
upViewport()

## Top Right
pushViewport(viewport(x=0.49, y=0.40, width=.49, height=.6,
                      just=c("left", "bottom")))
drawDetails.lattice(A2[[2]])
upViewport()


```

Prior to randomization, investigators spent time using a web application built using the \texttt{Shiny} package in \texttt{R}. The purpose of this is to help investigators explore the strengths of matching on multiple variables to see what gives advantageous balance across relevant baseline variables. We recommend deciding on ideal and achievable maximum differences in study arms and using many combinations of strengths of matching until one is found which ensures randomization is likely to be within those bounds. In the well-known children's fable The Three Bears, Goldilocks tries three bowls of porridge, one is too hot, the other too cold, and the third is just right [@3Bears]. We recommend a similar procedure applied to strengths of matching, with perhaps more attempts.

Figure 1 demonstrates this process using three variables: attributable patient days per month, Staphylococcus aureus rate, and MRSA rate. After initial explorations on the web application, investigators agreed that an ideal maximum mean differences in treatment and control arms for these variables were: 80 attributable patient days per month, 15% difference in Staphylococcus aureus infection rates, and 15% difference in MRSA rate. The graph on the top left shows no strength of matching on any of these variables, the values exceed the maximums in the second and third axis: there is a reasonable chance that if randomization occurred with this weighting the Staphylococcus aureus and MRSA rate would be above the desired maximum mean differences in treatment and control arms. To rectify this, positive strengths must be added. In the top-right graph a strength of 8 has been applied to the Staphylococcus aureus rate. In this graph, the matching of hospitals is strongly adjusted so that hospitals with similar Staphylococcus aureus rates are paired. This results in low mean difference between the treatment and control arms in that variable. The values on the middle axis are below the maximum value: if randomization occurred using these strengths we are likely to get suitable balance in this variable. Unfortunately, there is a penalty. Hospitals with similar Staphylococcus aureus rates do not have similar attributable patient days per month and MRSA rates, which results in these values exceeding the maximum. In particular, our investigators felt that the chance of attaining MRSA rates above 15% were too high for these strengths. The bottom plot shows the possible mean balances used in the actual randomization for these three variables, the strengths of matching for each variable were 1, 4, and 2, respectively. In all graphs, the black line indicates the mean value of all points on each axis. 

Our investigators used this approach with 16 variables. After trying many strengths of matching they found one with the best balance between treatment and control arms for the variables of importance. When the trial was randomized we used these strength to match hospitals in the study, then randomized the first member of each match to either treatment control. The results can be seen in Figure 2. 

One variable in Figure 2, median length of stay, is fixed on one value. For this variable, all computed randomizations had the same mean difference in median length of stay in the control and treatment arm. When randomization occurs we can be almost certain that we will attain this mean difference in median length of stay in the treatment and control arms.

```{r FullGraph, echo=FALSE, fig.align='center', fig.show='asis', message=FALSE, warning=FALSE, cache.lazy=TRUE}
## install.packages("designmatch")
library(designmatch)

## The first is used for its parallelplot() function
## The second to add limits to all y-axises, and helps
## us make the final plot.
library(lattice)
library(grid)

## Used to read in the data
## install.packages("readxl")
library(readxl)
library(shiny)

## Used to get the right colors in
library("RColorBrewer")
mypalette<-brewer.pal(11, "RdBu")

## Functions that are used to copy and paste the parallelplot
## into soemthing that talks to viewports.
latticeGrob <- function(p){
  grob(p=p, cl="lattice")
}

drawDetails.lattice <- function(x, recording=FALSE){
  lattice:::plot.trellis(x$p, newpage=FALSE)
} 

nmatch <- function (dist_mat, subset_weight = NULL, total_pairs = NULL, 
    mom = NULL, exact = NULL, near_exact = NULL, fine = NULL, 
    near_fine = NULL, near = NULL, far = NULL, solver = NULL) 
{
    if (is.null(mom)) {
        mom_covs = NULL
        mom_tols = NULL
        mom_targets = NULL
    }
    else {
        mom_covs = mom$covs
        mom_tols = mom$tols
        mom_targets = mom$targets
    }
    if (is.null(exact)) {
        exact_covs = NULL
    }
    else {
        exact_covs = exact$covs
    }
    if (is.null(near_exact)) {
        near_exact_covs = NULL
        near_exact_devs = NULL
    }
    else {
        near_exact_covs = near_exact$covs
        near_exact_devs = near_exact$devs
    }
    if (is.null(fine)) {
        fine_covs = NULL
    }
    else {
        fine_covs = fine$covs
    }
    if (is.null(near_fine)) {
        near_fine_covs = NULL
        near_fine_devs = NULL
    }
    else {
        near_fine_covs = near_fine$covs
        near_fine_devs = near_fine$devs
    }
    if (is.null(near)) {
        near_covs = NULL
        near_pairs = NULL
        near_groups = NULL
    }
    else {
        near_covs = near$covs
        near_pairs = near$pairs
        near_groups = near$groups
    }
    if (is.null(far)) {
        far_covs = NULL
        far_pairs = NULL
        far_groups = NULL
    }
    else {
        far_covs = far$covs
        far_pairs = far$pairs
        far_groups = far$groups
    }
    if (is.null(solver)) {
        solver = "glpk"
        t_max = 60 * 15
        approximate = 1
    }
    else {
        t_max = solver$t_max
        approximate = solver$approximate
        trace = solver$trace
        round_cplex = solver$round_cplex
        solver = solver$name
    }
    n_tot = nrow(dist_mat)
    n_dec = (n_tot * (n_tot - 1)) - sum(1:(n_tot - 1))
    if (is.null(subset_weight)) {
        subset_weight = 0
    }
    cvec = t(dist_mat)[lower.tri(dist_mat)] - (subset_weight * 
        rep(1, n_dec))
    rows_far = NULL
    cols_far = NULL
    vals_far = NULL
    rows_near = NULL
    cols_near = NULL
    vals_near = NULL
    rows_mom = NULL
    cols_mom = NULL
    vals_mom = NULL
    rows_exact = NULL
    cols_exact = NULL
    vals_exact = NULL
    rows_near_exact = NULL
    cols_near_exact = NULL
    vals_near_exact = NULL
    rows_fine = NULL
    cols_fine = NULL
    vals_fine = NULL
    rows_near_fine = NULL
    cols_near_fine = NULL
    vals_near_fine = NULL
    rows_n = NULL
    cols_n = NULL
    vals_n = NULL
    rows_target = NULL
    cols_target = NULL
    vals_target = NULL
    rows_nbm = sort(rep(1:n_tot, n_tot - 1))
    temp = matrix(0, nrow = n_tot, ncol = n_tot)
    temp[lower.tri(temp)] = 1:n_dec
    temp = temp + t(temp)
    diag(temp) = NA
    cols_nbm = as.vector(t(temp))
    cols_nbm = cols_nbm[!is.na(cols_nbm)]
    vals_nbm = rep(1, (n_tot - 1) * n_tot)
    row_count = max(rows_nbm)
    rows_ind_far_pairs = list()
    if (!is.null(far_covs)) {
        rows_far = NULL
        cols_far = NULL
        vals_far = NULL
        n_far_covs = ncol(far_covs)
        for (j in 1:n_far_covs) {
            far_cov = far_covs[, j]
            if (!is.null(far_groups)) {
                far_group = far_groups[j]
                row_ind_far_all = rep(row_count + 1, n_dec)
                col_ind_far_all = rep(1:n_dec, 1)
                i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
                aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, 
                  byrow = F)
                j_ind = aux[lower.tri(aux)]
                vals_far_all = far_cov[i_ind] - far_cov[j_ind] - 
                  (far_group * rep(1, n_dec))
                row_count = max(row_ind_far_all)
            }
            if (!is.null(far_pairs)) {
                far_pair = far_pairs[j]
                aux = abs(outer(far_cov, far_cov, FUN = "-"))
                temp = as.vector(matrix(t(aux)[lower.tri(aux)], 
                  nrow = 1, byrow = TRUE))
                cols_ind_far_pairs = which(temp < far_pair)
                if (length(cols_ind_far_pairs) > 0) {
                  rows_ind_far_pairs[[j]] = row_count + (1:length(cols_ind_far_pairs))
                  vals_far_pairs = rep(1, length(cols_ind_far_pairs))
                  row_count = max(rows_ind_far_pairs[[j]])
                }
                if (length(cols_ind_far_pairs) == 0) {
                  cols_ind_far_pairs = NULL
                  rows_ind_far_pairs[[j]] = -1
                  vals_far_pairs = NULL
                }
            }
            if (!is.null(far_groups) && is.null(far_pairs)) {
                rows_far = c(rows_far, row_ind_far_all)
                cols_far = c(cols_far, col_ind_far_all)
                vals_far = c(vals_far, vals_far_all)
            }
            if (is.null(far_groups) && !is.null(far_pairs) && 
                rows_ind_far_pairs[[j]] != -1) {
                rows_far = c(rows_far, rows_ind_far_pairs[[j]])
                cols_far = c(cols_far, cols_ind_far_pairs)
                vals_far = c(vals_far, vals_far_pairs)
            }
            if (!is.null(far_groups) && !is.null(far_pairs) && 
                rows_ind_far_pairs[[j]] != -1) {
                rows_far = c(rows_far, row_ind_far_all, rows_ind_far_pairs[[j]])
                cols_far = c(cols_far, col_ind_far_all, cols_ind_far_pairs)
                vals_far = c(vals_far, vals_far_all, vals_far_pairs)
            }
            if (!is.null(far_groups) && !is.null(far_pairs) && 
                rows_ind_far_pairs[[j]] == -1) {
                rows_far = c(rows_far, row_ind_far_all)
                cols_far = c(cols_far, col_ind_far_all)
                vals_far = c(vals_far, vals_far_all)
            }
        }
    }
    rows_ind_near_pairs = list()
    if (!is.null(near_covs)) {
        rows_near = NULL
        cols_near = NULL
        vals_near = NULL
        n_near_covs = ncol(near_covs)
        for (j in 1:n_near_covs) {
            near_cov = near_covs[, j]
            if (!is.null(near_groups)) {
                near_group = near_groups[j]
                row_ind_near_all = rep(row_count + 1, n_dec)
                col_ind_near_all = rep(1:n_dec, 1)
                i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
                aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, 
                  byrow = F)
                j_ind = aux[lower.tri(aux)]
                vals_near_all = near_cov[i_ind] - near_cov[j_ind] - 
                  (near_group * rep(1, n_dec))
                row_count = max(row_ind_near_all)
            }
            if (!is.null(near_pairs)) {
                near_pair = near_pairs[j]
                aux = abs(outer(near_cov, near_cov, FUN = "-"))
                temp = as.vector(matrix(t(aux)[lower.tri(aux)], 
                  nrow = 1, byrow = TRUE))
                cols_ind_near_pairs = which(temp > near_pair)
                if (length(cols_ind_near_pairs) > 0) {
                  rows_ind_near_pairs[[j]] = row_count + (1:length(cols_ind_near_pairs))
                  vals_near_pairs = rep(1, length(cols_ind_near_pairs))
                  row_count = max(rows_ind_near_pairs[[j]])
                }
                if (length(cols_ind_near_pairs) == 0) {
                  cols_ind_near_pairs = NULL
                  rows_ind_near_pairs[[j]] = -1
                  vals_near_pairs = NULL
                }
            }
            if (!is.null(near_groups) && is.null(near_pairs)) {
                rows_near = c(rows_near, row_ind_near_all)
                cols_near = c(cols_near, col_ind_near_all)
                vals_near = c(vals_near, vals_near_all)
            }
            if (is.null(near_groups) && !is.null(near_pairs) && 
                rows_ind_near_pairs[[j]] != -1) {
                rows_near = c(rows_near, rows_ind_near_pairs[[j]])
                cols_near = c(cols_near, cols_ind_near_pairs)
                vals_near = c(vals_near, vals_near_pairs)
            }
            if (!is.null(near_groups) && !is.null(near_pairs) && 
                rows_ind_near_pairs[[j]] != -1) {
                rows_near = c(rows_near, row_ind_near_all, rows_ind_near_pairs[[j]])
                cols_near = c(cols_near, col_ind_near_all, cols_ind_near_pairs)
                vals_near = c(vals_near, vals_near_all, vals_near_pairs)
            }
            if (!is.null(near_groups) && !is.null(near_pairs) && 
                rows_ind_near_pairs[[j]] == -1) {
                rows_near = c(rows_near, row_ind_near_all)
                cols_near = c(cols_near, col_ind_near_all)
                vals_near = c(vals_near, vals_near_all)
            }
        }
    }
    if (!is.null(mom_covs) & is.null(mom_targets)) {
        rows_mom_1 = NA
        cols_mom_1 = NA
        vals_mom_1 = NA
        rows_mom_2 = NA
        cols_mom_2 = NA
        vals_mom_2 = NA
        n_covs_m = ncol(mom_covs)
        for (i in 1:n_covs_m) {
            cov_m = mom_covs[, i]
            rows_mom_1 = c(rows_mom_1, rep(row_count + i, n_dec))
            cols_mom_1 = c(cols_mom_1, 1:n_dec)
            i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
            aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, byrow = F)
            j_ind = aux[lower.tri(aux)]
            vals_mom_1 = c(vals_mom_1, cov_m[i_ind] - cov_m[j_ind] - 
                (mom_tols[i] * rep(1, n_dec)))
        }
        rows_mom_1 = rows_mom_1[-1]
        cols_mom_1 = cols_mom_1[-1]
        vals_mom_1 = vals_mom_1[-1]
        row_count = max(rows_mom_1)
        for (i in 1:n_covs_m) {
            cov_m = mom_covs[, i]
            rows_mom_2 = c(rows_mom_2, rep(row_count + i, n_dec))
            cols_mom_2 = c(cols_mom_2, 1:n_dec)
            i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
            aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, byrow = F)
            j_ind = aux[lower.tri(aux)]
            vals_mom_2 = c(vals_mom_2, cov_m[j_ind] - cov_m[i_ind] - 
                (mom_tols[i] * rep(1, n_dec)))
        }
        rows_mom_2 = rows_mom_2[-1]
        cols_mom_2 = cols_mom_2[-1]
        vals_mom_2 = vals_mom_2[-1]
        rows_mom = c(rows_mom_1, rows_mom_2)
        cols_mom = c(cols_mom_1, cols_mom_2)
        vals_mom = c(vals_mom_1, vals_mom_2)
        row_count = max(rows_mom)
    }
    if (!is.null(mom_covs) & !is.null(mom_targets)) {
        n_covs_m = ncol(mom_covs)
        rows_target = sort(rep(1:(4 * n_covs_m) + row_count, 
            n_dec))
        for (i in 1:n_covs_m) {
            cov_m = mom_covs[, i]
            cols_target = c(cols_target, rep(1:n_dec, 4))
            i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
            aux = matrix(rep(1:n_tot, n_tot), nrow = n_tot, byrow = F)
            j_ind = aux[lower.tri(aux)]
            vals_target = c(vals_target, cov_m[i_ind] - (mom_targets[i] + 
                mom_tols[i]), -1 * cov_m[i_ind] + (mom_targets[i] - 
                mom_tols[i]), cov_m[j_ind] - (mom_targets[i] + 
                mom_tols[i]), -1 * cov_m[j_ind] + (mom_targets[i] - 
                mom_tols[i]))
        }
        row_count = max(rows_target)
    }
    rows_exact = numeric()
    cols_exact = numeric()
    vals_exact = numeric()
    if (!is.null(exact_covs)) {
        n_exact_cats = ncol(exact_covs)
        for (i in 1:n_exact_cats) {
            rows_exact = c(rows_exact, rep(row_count + i, n_dec))
            cols_exact = c(cols_exact, 1:n_dec)
            dist_exact_cov = abs(outer(exact_covs[, i], exact_covs[, 
                i], "-"))
            vals_exact = c(vals_exact, dist_exact_cov[lower.tri(dist_exact_cov)])
        }
        row_count = max(rows_exact)
    }
    rows_near_exact = numeric()
    cols_near_exact = numeric()
    vals_near_exact = numeric()
    if (!is.null(near_exact_covs)) {
        n_near_exact_cats = ncol(near_exact_covs)
        for (i in 1:n_near_exact_cats) {
            rows_near_exact = c(rows_near_exact, rep(row_count + 
                j, n_dec))
            cols_near_exact = c(cols_near_exact, 1:n_dec)
            dist_near_exact_cov = abs(outer(near_exact_covs[, 
                i], near_exact_covs[, i], "-"))
            vals_near_exact = c(vals_near_exact, dist_near_exact_cov[lower.tri(dist_near_exact_cov)])
        }
        row_count = max(rows_near_exact)
    }
    if (!is.null(fine_covs)) {
        fine_covs_2 = rep(NA, nrow(fine_covs))
        n_fine_covs = ncol(fine_covs)
        j = 1
        for (i in 1:n_fine_covs) {
            aux = factor(fine_covs[, i])
            fine_covs_2 = cbind(fine_covs_2, diag(nlevels(aux))[aux, 
                ])
            if (j == 1) {
                fine_covs_2 = fine_covs_2[, -1]
            }
            j = j + 1
        }
        n_fine_cats = ncol(fine_covs_2)
        j = 1
        for (i in 1:n_fine_cats) {
            rows_fine = c(rows_fine, rep(row_count + j, n_dec))
            cols_fine = c(cols_fine, 1:n_dec)
            dist_fine_cov = outer(fine_covs_2[, i], fine_covs_2[, 
                i], "-")
            dist_fine_cov = t(dist_fine_cov)
            vals_fine = c(vals_fine, dist_fine_cov[lower.tri(dist_fine_cov)])
            if (j == 1) {
                rows_fine = rows_fine[-1]
                cols_fine = cols_fine[-1]
                vals_fine = vals_fine[-1]
            }
            j = j + 1
        }
        row_count = max(rows_fine)
    }
    if (!is.null(near_fine_covs)) {
        near_fine_covs_2 = rep(NA, nrow(near_fine_covs))
        n_near_fine_covs = ncol(near_fine_covs)
        j = 1
        for (i in 1:n_near_fine_covs) {
            aux = factor(near_fine_covs[, i])
            near_fine_covs_2 = cbind(near_fine_covs_2, diag(nlevels(aux))[aux, 
                ])
            if (j == 1) {
                near_fine_covs_2 = near_fine_covs_2[, -1]
            }
            j = j + 1
        }
        n_near_fine_cats = ncol(near_fine_covs_2)
        j = 1
        for (i in 1:n_near_fine_cats) {
            for (h in 1:2) {
                rows_near_fine = c(rows_near_fine, rep(row_count + 
                  j, n_dec))
                cols_near_fine = c(cols_near_fine, 1:n_dec)
                dist_near_fine_cov = outer(near_fine_covs_2[, 
                  i], near_fine_covs_2[, i], "-")
                dist_near_fine_cov = t(dist_near_fine_cov)
                vals_near_fine = c(vals_near_fine, dist_near_fine_cov[lower.tri(dist_near_fine_cov)])
                if (j == 1) {
                  rows_near_fine = rows_near_fine[-1]
                  cols_near_fine = cols_near_fine[-1]
                  vals_near_fine = vals_near_fine[-1]
                }
                j = j + 1
            }
        }
        row_count = max(rows_near_fine)
    }
    if (!is.null(total_pairs)) {
        rows_n = rep(row_count + 1, n_dec)
        cols_n = 1:n_dec
        vals_n = rep(1, n_dec)
        row_count = max(rows_n)
    }
    rows = c(rows_nbm, rows_far, rows_near, rows_mom, rows_target, 
        rows_exact, rows_near_exact, rows_fine, rows_near_fine, 
        rows_n)
    cols = c(cols_nbm, cols_far, cols_near, cols_mom, cols_target, 
        cols_exact, cols_near_exact, cols_fine, cols_near_fine, 
        cols_n)
    vals = c(vals_nbm, vals_far, vals_near, vals_mom, vals_target, 
        vals_exact, vals_near_exact, vals_fine, vals_near_fine, 
        vals_n)
    aux = cbind(rows, cols, vals)[order(cols), ]
    cnstrn_mat = simple_triplet_matrix(i = aux[, 1], j = aux[, 
        2], v = aux[, 3])
    Amat = cnstrn_mat
    bvec = rep(1, length(table(rows_nbm)))
    if (!is.null(far_covs)) {
        n_far_covs = ncol(far_covs)
        for (j in 1:n_far_covs) {
            if (!is.null(far_groups)) {
                bvec = c(bvec, rep(0, 1))
            }
            if (!is.null(far_pairs) && rows_ind_far_pairs[[j]] != 
                -1) {
                bvec = c(bvec, rep(0, length(table(rows_ind_far_pairs[[j]]))))
            }
        }
    }
    if (!is.null(near_covs)) {
        n_near_covs = ncol(near_covs)
        for (j in 1:n_near_covs) {
            if (!is.null(near_groups)) {
                bvec = c(bvec, rep(0, 1))
            }
            if (!is.null(near_pairs) && rows_ind_near_pairs[[j]] != 
                -1) {
                bvec = c(bvec, rep(0, length(table(rows_ind_near_pairs[[j]]))))
            }
        }
    }
    bvec = c(bvec, rep(0, length(table(rows_mom))))
    bvec = c(bvec, rep(0, length(table(rows_target))))
    if (!is.null(exact_covs)) {
        bvec = c(bvec, rep(0, ncol(exact_covs)))
    }
    if (!is.null(near_exact_covs)) {
        bvec = c(bvec, near_exact_devs)
    }
    bvec = c(bvec, rep(0, length(table(rows_fine))))
    if (!is.null(near_fine_covs)) {
        bvec_8_aux = rep(NA, length(rows_near_fine))
        bvec_8_aux[seq(1, length(rows_near_fine), 2)] = -near_fine_devs
        bvec_8_aux[seq(2, length(rows_near_fine), 2)] = near_fine_devs
        bvec = c(bvec, bvec_8_aux)
    }
    if (!is.null(total_pairs)) {
        bvec = c(bvec, total_pairs)
    }
    ub = rep(1, n_dec)
    sense = rep("L", length(table(rows_nbm)))
    if (!is.null(far_covs)) {
        n_far_covs = ncol(far_covs)
        for (j in 1:n_far_covs) {
            if (!is.null(far_groups)) {
                sense = c(sense, rep("G", 1))
            }
            if (!is.null(far_pairs) && rows_ind_far_pairs[[j]] != 
                -1) {
                sense = c(sense, rep("E", length(table(rows_ind_far_pairs[[j]]))))
            }
        }
    }
    if (!is.null(near_covs)) {
        n_near_covs = ncol(near_covs)
        for (j in 1:n_near_covs) {
            if (!is.null(near_groups)) {
                sense = c(sense, rep("L", 1))
            }
            if (!is.null(near_pairs) && rows_ind_near_pairs[[j]] != 
                -1) {
                sense = c(sense, rep("E", length(table(rows_ind_near_pairs[[j]]))))
            }
        }
    }
    sense = c(sense, rep("L", length(table(rows_mom))))
    sense = c(sense, rep("L", length(table(rows_target))))
    if (!is.null(exact_covs)) {
        sense = c(sense, rep("E", ncol(exact_covs)))
    }
    if (!is.null(near_exact_covs)) {
        sense = c(sense, rep("L", ncol(near_exact_covs)))
    }
    sense = c(sense, rep("E", length(table(rows_fine))))
    sense = c(sense, rep(c("G", "L"), length(table(rows_near_fine))/2))
    sense = c(sense, rep("E", length(total_pairs)))
    if (approximate == 1) {
        var_type = rep("C", n_dec)
    }
    else {
        var_type = rep("B", n_dec)
    }
    if (solver == "glpk") {
        dir = rep(NA, length(sense))
        dir[sense == "E"] = "=="
        dir[sense == "L"] = "<="
        dir[sense == "G"] = ">="
        bound = list(lower = list(ind = c(1:length(ub)), val = rep(0, 
            length(ub))), upper = list(ind = c(1:length(ub)), 
            val = ub))
        ptm = proc.time()
        out = Rglpk_solve_LP(cvec, Amat, dir, bvec, bounds = bound, 
            types = var_type, max = FALSE)
        time = (proc.time() - ptm)[3]
        if (out$status != 0) {
            cat(format("  Error: problem infeasible!"), "\n")
            obj_val = NA
            obj_dist_mat = NA
            id_1 = NA
            id_2 = NA
            group_id = NA
            time = NA
        }
        if (out$status == 0) {
            if (approximate == 1) {
                rel = .relaxation_n(n_tot, out$solution, dist_mat, 
                  subset_weight, "glpk", round_cplex, trace)
                out$solution = rel$sol
                out$optimum = rel$obj
                time = time + rel$time
            }
            i_ind = rep(1:(n_tot - 1), (n_tot - 1):1)
            aux = matrix(1:n_tot, nrow = n_tot, ncol = n_tot)
            j_ind = aux[lower.tri(aux)]
            group_1 = i_ind[out$solution == 1]
            group_2 = j_ind[out$solution == 1]
            max_groups = apply(cbind(group_1, group_2), 1, max)
            id_1 = group_1[max_groups <= n_tot]
            id_2 = group_2[max_groups <= n_tot]
            group_id_1 = 1:(length(id_1))
            group_id_2 = 1:(length(id_2))
            group_id = c(group_id_1, group_id_2)
            obj_val = out$optimum
            obj_dist_mat = sum(t(dist_mat)[lower.tri(dist_mat)] * 
                out$solution)
        }
    }
    return = list(obj_total = obj_val, obj_dist_mat = obj_dist_mat, 
        id_1 = id_1, id_2 = id_2, group_id = group_id, time = time)
}


makePlot = function(V, P, D, Tm, NVs, cols){
  ## Including means of columns and indicator that tells
  ## whether or not it is to be included in plot as mean (light blue)
  ## or regular data point
  CM = colMeans(P[[3]])
  G = c(rep("A", Tm), "B")
  P[[3]] = cbind(rbind(P[[3]], CM), G)
  
  ## Setting the upper values for the parallelplot
  upper <- as.numeric(P[[2]][,"Maxs"])
  
  ## draw = FALSE, to get rid of x axis labels for hists
  print(parallelplot(~P[[3]][cols], P[[3]], groups = G, horizontal.axis = FALSE,
                    scales = list(x = list(rot = 90, cex = 0.5), ## draw = FALSE, 
                                  y = list(draw = FALSE)),
                    col = c("grey50", "black"), lwd = c(1,3),
                    lower = 0, upper = upper, main = "Possible Randomizations",
                    par.settings = list(axis.line = list(col = 0)),
                    panel = function(...) {
                      panel.parallel(...)
                      ## If plotting with histograms underneath take out
                      ## first grid.text
                      grid.text(0,
                                 x=unit(1:length(cols), "native"),
                                 y=unit(2, "mm"), just="bottom",
                                 gp=gpar(col="grey", cex=.7))
                      grid.text(upper,
                                x=unit(1:length(cols), "native"),
                                y=unit(1, "npc") - unit(2, "mm"), 
                                just="top",
                                gp=gpar(col="grey", cex=.7))
                      
                    }))
}

# Function that does the randomizations
# M is the number of randomizations
# vars is a list of parameters which includes
# D is the data set from which the columns to be matched on are drawn
# names is the file name
# S is the optimisation method


make.Ks = function(M, vars, D, Plot, S, ToC){
  r.I = length(vars)
  No.cols = length(vars[[1]])
  I = matrix(NA, nrow = r.I, ncol = No.cols)
  for(i in 1:r.I) I[i,] = unlist(vars[[i]])
  
  dimnames(I) = list(rep(NULL, r.I), c("cols", "w", "L", "Mins", "Maxs"))
  
  ## No. of participant hospitals
  N = dim(D)[1]
  
  mymat = sdmat = matrix(NA, nrow = N, ncol = r.I)
  
  ## Combining columns
  for(i in 1:r.I) mymat[,i] = D[,I[i,"cols"]]
  
  ## Standardising and adding weights
  col.means = colMeans(mymat)
  col.sds = apply(mymat, 2, sd)
  w = as.numeric(I[,"w"])
  for(i in 1:r.I) sdmat[,i] = 
    w[i]*(mymat[,i] - col.means[i])/col.sds[i]
  
  ## Making the distance matrix
  dist_mat_obj = dist(sdmat, diag = TRUE, upper = TRUE)
  dist_mat = as.matrix(dist_mat_obj)
  
  ## Telling the computer wich method to use to solve
  t_max = 60*5
  solver = S
  approximate = 0
  solver = list(name = solver, t_max = t_max, approximate = approximate, 
                round_cplex = 0, trace_cplex = 0)
  
  ## Solving
  out = nmatch(dist_mat = dist_mat, total_pairs = floor(N/2), 
               solver = solver, subset_weight = NULL)
  
  # These guys have the row numbers of the matched pairs
  id_1 = out$id_1  
  id_2 = out$id_2 
  
  ## If there are any leftovers they get assigned where
  ## the user wants them assigned
  X = dim(D)[1] %% 2 == 0
  if(X == FALSE) {LO = sum(1:dim(D)[1]) - sum(id_1, id_2)}
  
  ## Will be used in the loop below - a place to put Ks, 
  ## filled below, then graphed after using parallel coordinate plot
  Ks = matrix(NA, nrow = M, ncol = r.I)
  
  ## Used to run the loop below
  M.seq = seq_along(1:M)
  
  for(i in M.seq){
    ## Randomising once - 0 ctl, 1 trt, (subtract one because when 
    ## making Trt below the middle step has to identify the length of 
    ## Trt or it gets fussy and won't add on the last value)
    R = replicate(length(id_1), rbinom(1, size = 1, prob = 0.5))
    S = 1 - R
    
    ## The 1 is for the leftover row that goes to the trt arm
    Trt = c(R, S)
    
    ## Making the data, TA is included and goes to the trt arm
    if (X == TRUE) {
      Dt = data.frame(mymat[c(id_1, id_2),], Trt)
    } else {
      Dt = data.frame(mymat[c(id_1, id_2, LO),], c(Trt, (ToC - 1)))
    }
    
    ## Picking out trt and ctl covariates
    Trt.CV = Dt[which(Trt == 1), 1:r.I]
    Ctl.CV = Dt[-which(Trt == 1), 1:r.I]
    
    Ks[i,] = abs((apply(Trt.CV, 2, sum) - 
                    apply(Ctl.CV, 2, sum))/N)
  }## ending for i in M.seq
  
  ## So the labels in parcoord come out nicely
  ## NEW colnames here
  colnames(Ks) = I[,'L']
  
  P = list()
  P[[1]] = cbind(id_1, id_2)
  P[[2]] = I
  P[[3]] = as.data.frame(Ks)
  
  P
} ## closing the function make.Ks

D = read_excel("ArtData.xlsx",  sheet=2)[1:137,]
D = as.data.frame(D)

## Final weighting for randomization
V = list(list("Attrib_Days_Month", 1, "Pt Days", 0, 80),
         list("Outcome_Saureus", 4, "S auer Rate", 0, 0.15),
         list("Outcome_MRSA", 2, "MRSA Rate", 0, 0.15),
         list("All_Blood", 4, "All Blood", 0, 0.15),
         list("Mup-R", 2, "Mup-R", 0, 0.02),
         list("Hx_MRSA", 1,"Hx MRSA", 0, 0.01),
         list("Mup_Adherence", 1, "Mup Adherence", 0, 0.05),
         list("CHG_Adherence", 1,"CHG Adherence", 0, 0.05),
         list("Median_LOS", 3,"Median LOS", 0, 0.05),
         list("Elix_Score", 1,"Comorbidity Score", 0, 0.15),
         list("Medicaid", 0,"Medicaid", 0, 0.02),
         list("PCR_Blood", 0,"PCR_Blood", 0, 0.1),
         list("DC_SNF", 0,"DC SNF", 0, 0.02),
         list("Surgery", 1,"Surgery", 0, 0.02),
         list("Onc_BMT_Trp", 2,"Onc_BMT_Trp", 0, 0.1),
         list("BMT_Trp", 0,"BMT_Trp", 0, 0.1)
         )

## Making data 
## The leftover is sent to the treatment arm (ToC = 2)
P = make.Ks(M = 300, D = D, Plot = "noscreen", vars = V, S = "glpk", ToC = 2)

## Plotting
makePlot(V = V, P = P, D = D, Tm = 300, NVs = 2, cols = 1:length(V))

```


## Discussion
While the Goldilocks approach to randomizing does not ensure balance in the treatment and control arms, it is a tool that provides investigators with a method to explore strengths of matching that impact matching and balance. We encourage investigators that utilize CRTs to use this method prior to randomizing to find more balance in treatment and control arms.

In matching for SWAPOUT, we standardized all $16$ variables by subtracting the mean from each value, then dividing by the standard deviation. To simplify notation, we reduced the algorithm as this impacts the initial plot, but the process remains the same. In SWAPOUT, the $d_{ij}$ is derived from the nonstandardized data, the standardized data was solely used for matching.

If investigators would like to use this method on studies with more than $2$ arms, $d_{ij}$ can be found for multiple combinations of arms and multiple plots can be assessed. 

Future work in this area includes publishing a \texttt{Shiny} web application for investigators to use. This application will eventually be an interactive plot that enables users to click on each axis and view where low and high draws of that variable fall for other variables. In some cases, our investigators find that matching on 1 variable seems to give suitable balance throughout. 

## Appendix
A more formal explanation of the variables here, in table format, to be checked with Susan.

Variable  | Description
----------| ---------------- 
Pt Days | 
S auer Rate | 
MRSA Rate |
All Blood |
Mup-R |
Hx MRSA |
Mup Adherence |
CHG Adherence |
Median LOS |
Medicaid |
Comorbidity Score |
Medicaid |
PCR Blood |
DC SNF |
Surgery |
Onc_BMT_Trp |
BMT_Trp |

Table 2: Thorough description of baseline variables used in this paper.

