---
title: "$A$ $Priori$ Matching in Cluster Randomized Trials"
author:
- name: S. Gwynn Sturdevant, Ken Kleinman
  affiliation: University of Massachusetts Amherst
output: 
    pdf_document:
      citation_package: natbib
      keep_tex: true
      fig_caption: true
      latex_engine: pdflatex
bibliography: bibliography.bib
documentclass: sagej
abstract: "Matching in cluster-randomized trials (CRTs) is important, but there is no best practice. When baseline data is available, we suggest a technique which can be used to identify the best weighting of pertinent variables to enable more balance between treatment and control groups. This technique involves standardizing the variables, computing the Mahalanobis distance, use matching to find pairs, then randomizing and rerandomizing. For each randomiziation we compute the difference in the treatment and control arms for each variable and plot these on a parallel-coordinates plot. Investigators can compare plots to identify weighting for advantageous randomization schema."
keywords: "cluster-randomized trials, matching"
date: "`r format(Sys.time(), '%B %d, %Y')`"
---

## Introduction
To determine the efficacy of a treatment, individually randomized trials (IRTs) with blinding are the "strongest study design available" [@gatsonis2017methods]. Unfortunately, cost and study design amongst other things mean some interventions can not be randomized on an individual level. For example, education researchers decide to determine if training elementary school teachers in a reading program will affect literacy skills in third graders. Randomizing each third grader to treatment or control would not be suitable, as this equates to randomly allocating students to a teacher; rural schools tend to be smaller with only one teacher which further complicates matters. Instead, researchers may choose to randomize teachers, schools, or counties to determine any differences between the two arms of the study. Trials where groups are randomized are called cluster randomized trials (CRTs). Three reasons for conducting a CRTs are: (i) implementation occurs at the cluster level, (ii) to avoid contamination, and (iii) to measure intervention effects among cluster members who do not receive treatment [@balzer2012match; @CRTrials2009]. CRTs are "the gold standard when allocation of identifiable groups is necessary" [@murray2004design].

Many authors debate matching in CRTs [@balzer2012match; @CRTrials2009; @gatsonis2017methods; @diehr1995breaking; @murray1998design; @imai2009essential;
@PMVsStrat; @donner2007merits; @klar1997merits; @donner2000design; @martin1993effect]. Murray argues that "the choice of matching or stratification [of] factors is critical to the success of the procedure" [@murray1998design]. Some agree that caution must be used when matching a small number of clusters due to the decrease in power [@donner2000design; @klar1997merits; @balzer2012match; @martin1993effect]. Breaking matches in the analysis stage addresses this [@diehr1995breaking], but perhaps only when there are a small number of large clusters [@donner2007merits].  Others argue that matching is effective in a small number of clusters as it "increases the chance of the intervention groups being well-balanced" [@donner2007merits]. Imai et al argue that not matching, in small or large sample, is "equivalent to discarding a considerable fraction of one's data" [@imai2009essential]. However, in one trial "matching actually led to a loss in statistical efficiency" [@manun1994influence, @donner2000design]. 

Despite all this debate few authors discuss methodologies to support the matching process [@raab2001balance]. An alternative to matching suggests balancing criterion based on conditioned means of covariates [@raab2001balance].  ZUBIZARRETA [@greevy2004optimal]. Our article is an extension of methods introduced in Chapter 4 of Methods in Comparative Effectiveness Research [@gatsonis2017methods]. We suggest a method suitable for $a$ $priori$ matching using baseline data. In section 2 <!-- #sec:Method  --> we outline our method, section 3 applies it to the SWAPOUT dataset, and section 4 is a brief discussion.

## Methods <!-- (#sec:Method) trying to get numbering to work not sure how yet  -->

To approach this complex topic of balancing randomization in CRTs we suggest a new approach. Our approach involves weighting variables of import, matching units using these weights, and randomizing many times to obtain a distribution of possibilities when official randomization occurs. Investigators assess these distributions to determine if possible randomizations are sufficiently balanced, if not, weighting is adjusted and the process begins again. The details follow.

The initial step involves prioritizing variables $(1, 2,..., m)$ from units $(1, 2, ..., n)$ to be randomized. We have

\begin{eqnarray*}
 \overline{V_1} & = & (v_{11}, v_{12},..., v_{1n})\\
 \overline{V_2} & = & (v_{21}, v_{22},..., v_{2n})\\
 \vdots & = & \vdots\\    
 \overline{V_m} & = & (v_{m1}, v_{m2},..., v_{mn}).\\
 \end{eqnarray*}
In addition, we use $\overline{w} = (w_{1}, w_{2},..., w_{m})$ to weight and standardize $(\overline{V_1}, \overline{V_2}, ..., \overline{V_m}).$ We have
\begin{eqnarray*}
v^{*}_{ij} = \frac{(v_{ij} - \frac{\sum _{k=1}^{n}v_{ik}}{n})*w_i}{sd(\overline{V_i})} 
 \end{eqnarray*}
where $sd(\overline{V_i})$ is the standard deviation of $\overline{V_i}.$ We now have
\begin{eqnarray*}
 \overline{V_1^*} & = & (v_{11}^*, v_{12}^*,..., v_{1n}^*)\\
 \overline{V_2^*} & = & (v_{21}^*, v_{22}^*,..., v_{2n}^*)\\
 \vdots & = & \vdots\\    
 \overline{V_m^*} & = & (v_{m1}^*, v_{m2}^*,..., v_{mn}^*).\\
 \end{eqnarray*}
 which we use to compute the Mahalanobois Distance matrix, **D**. From here we use the \texttt{nmatch} function in the \texttt{designmatch} [@doi] package in \texttt{R} [@nmatch]to find $\frac{n}{2}$ pairs if $n$ is even. If $n$ is odd, the remainder can be randomized to treatment or control per the principal investigator. Without loss of generality, we assume $n$ is even for the remainder of this paper and note that to include an odd $n$ either treatment or control groups will include one more set of priority variables.
 
Once the matching is completed and pairs found we return to using the raw data, as this will be used to assess the weighting scheme. We now have pairs $(\overline{V}_{11}, \overline{V}_{12}), (\overline{V}_{21}, \overline{V}_{22}), ..., (\overline{V}_{\frac{n}{2}1}, \overline{V}_{\frac{n}{2}2}).$ The first match in each pair will be randomized to either treatment or control using the \texttt{rbinom} function in \texttt{R}. Next, we subset $\overline{V}_1, \overline{V}_2, ..., \overline{V}_m$ into appropriate randomization subgroups: $\overline{V}_{1T}, \overline{V}_{1C}, \overline{V}_{2T}, \overline{V}_{2C},..., \overline{V}_{\frac{n}{2}T}, \overline{V}_{\frac{n}{2}C}$ where $\overline{V}_{iT} = (v_{i1}^T, v_{i2}^T,..., v_{in}^T),$ similarly for $\overline{V}_{iC}.$ Using these we find 
\begin{eqnarray*}
 k_j = | \sum_{i = 1}^{\frac{n}{2}}v_{ij}^T - \sum_{i = 1}^{\frac{n}{2}}v_{ij}^C | 
\end{eqnarray*}
THE ABOVE NEEDS A BETTER ABSOLUTE VALUE SYMBOL. for $j = 1, 2, ..., m.$ We randomize $N$ times and find $k_{lj}$ the difference in the two arms for the $j^{th}$ priority variable for each of the $l = 1, 2, ..., N$ re-randomizations. To assist analysis we draw a parallel coordinates plot where the $j^{th}$ axis plots $k_{lj}$ for $l = 1, 2, ..., N.$ If the principal investigator finds the possible differences too large for a priority variable $j$, increasing $w_j$ and re-running the above will update the matching to attain closer matches for this variable and lessen the differences. The penality in this process is that closer matches for variable $j$ are likely to imply reduced closeness in another variable, so compromises must be made.

## Results
To demonstrate the usefulness of this technique we present a brief summary of our randomization process using baseline data from the SWAPOUT trial (Cluster-randomized Non-inferiority Trial Comparing Mupirocin vs Iodophor for Nasal Decolonization of ICU Patients to Assess Impact on Staphylococcus aureus Clinical Cultures and All-cause Bloodstream Infection During Routine Chlorhexidine Bathing) [@SOTrial]. In this non-inferiority trial, the investigators are studying whether bathing with chlorhexidine gluconate and swabbing iodophor nasal swabs are inferior to bathing with the same and mupirocin nasal swabs. REDUCE trial ADD REF mupirocin nasal swabs and bathing with chlorhexidine reduced the MRSA Staphylococcus aureus (an antibiotic resistant infection) in Hospital Corporation of America intensive care units (ICU). However, physicians are reluctant to use mupirocin, an antibiotic, so investigators are swapping it with iodophor. 

Prior to randomization baseline data was collected for 20 months on the 137 hospitals. With this data, investigators met to prioritize baseline variables into several categories: primary, secondary, tertiary, quaternary, and not relevant to randomization. For this trial, the investigators decided that average monthly volume of patients with attributable time, average monthly attributable days, Staphylococcus aureus ICU-attributable cultures per 1,000 days, MRSA ICU-attributable cultures per 1,000 days, all pathogen ICU-attributable bacteremia cultures per 1,000 days, regional mupirocin resistance estimate, percent of admissions with MRSA diagnosis within a year, percent of mupirocin use admission to day 5, survey Chlorhexidine Glucominate were all of primary importance. Of secondary importance were median ICU length of stay, and mean elixhauser total score. Of tertiary importance were the percentage of ICU medicaid patients, and Polymerase chain reaction clinical specimen status. Lastly, percent admissions to skilled nursing facility (SNF), and if the ICU has specialty units for burn, trauma, etc. A list of abbreviations in the above order can be found in Table 1. To ease understanding, our initial discussion will involve 3 variables: Patient days, Staphylococcus auer rate, and MRSA rate.

Primary import  | Secondary import | Tetiary import | Quaternary import
--------------- | ---------------- | -------------- | -----------------
Pt Days | Median LOS | Medicaid
S auer Rate | Comorbidity Score | TALI WHAT ELSE??
MRSA Rate |
All Blood |
Mup-R |
Hx MRSA |
Mup Adherence |
CHG Adherence |

Table: Abbreviations of variables used to randomize

```{r IntroGraph, include=FALSE}
setwd("/Users/gwynn/Documents/PostDocKleinmanLab/Bins/Shiny/Again/sage_latex_template_3")
## install.packages("designmatch")
library(designmatch)

## The first is used for its parallelplot() function
## The second to add limits to all y-axises, and helps
## us make the final plot.
library(lattice)
library(grid)
library(gridGraphics)

## To add a table to the plot
library(plotrix)

## Used to read in the data
## install.packages("readxl")
library(readxl)
library(shiny)

## Used to get the right colors in
library("RColorBrewer")
mypalette<-brewer.pal(11, "RdBu")

## Functions that are used to copy and paste the parallelplot
## into soemthing that talks to viewports.
latticeGrob <- function(p){
  grob(p=p, cl="lattice")
}

drawDetails.lattice <- function(x, recording=FALSE){
  lattice:::plot.trellis(x$p, newpage=FALSE)
} 


makePlot = function(V, P, D, Tm, NVs, cols){
  ## Including means of columns and indicator that tells
  ## whether or not it is to be included in plot as mean (light blue)
  ## or regular data point
  CM = colMeans(P[[3]])
  G = c(rep("A", Tm), "B")
  P[[3]] = cbind(rbind(P[[3]], CM), G)
  
  ## Setting the upper values for the parallelplot
  upper <- as.numeric(P[[2]][,"Maxs"])
  
  ## Naming the columns
  CNs <- c(P[[2]][,'L'], "G")
  colnames(P[[3]]) = CNs
  
  p1 = parallelplot(~P[[3]][cols], P[[3]], groups = G, horizontal.axis = FALSE,
                    scales = list(x = list(rot = 90),  draw = FALSE,
                                  y = list(draw = FALSE)),
                    col = c("grey50", "black"), lwd = c(1,3),
                    lower = 0, upper = upper,
                    par.settings = list(axis.line = list(col = 0)),
                    panel = function(...) {
                      panel.parallel(...)
                      ## If plotting with histograms underneath take out
                      ## first grid.text
                      grid.text(0,
                                 x=unit(1:length(cols), "native"),
                                 y=unit(2, "mm"), just="bottom",
                                 gp=gpar(col="grey", cex=.7))
                      grid.text(upper[cols],
                                x=unit(1:length(cols), "native"),
                                y=unit(1, "npc") - unit(2, "mm"), 
                                just="top",
                                gp=gpar(col="grey", cex=.7))
                      
                    })
  
  A  = list()
  A[[2]] = latticeGrob(p1)
  
  A 
}

# Function that does the randomizations
# M is the number of randomizations
# vars is a list of parameters which includes
# D is the data set from which the columns to be matched on are drawn
# names is the file name
# S is the optimisation method

make.Ks = function(M, vars, D, Plot, S, ToC){
  r.I = length(vars)
  No.cols = length(vars[[1]])
  I = matrix(NA, nrow = r.I, ncol = No.cols)
  for(i in 1:r.I) I[i,] = unlist(vars[[i]])
  
  dimnames(I) = list(rep(NULL, r.I), c("cols", "w", "L", "Mins", "Maxs"))
  
  ## No. of participant hospitals
  N = dim(D)[1]
  
  mymat = sdmat = matrix(NA, nrow = N, ncol = r.I)
  
  ## Combining columns
  for(i in 1:r.I) mymat[,i] = D[,I[i,"cols"]]
  
  ## Standardising and adding weights
  col.means = colMeans(mymat)
  col.sds = apply(mymat, 2, sd)
  w = as.numeric(I[,"w"])
  for(i in 1:r.I) sdmat[,i] = 
    w[i]*(mymat[,i] - col.means[i])/col.sds[i]
  
  ## Making the distance matrix
  dist_mat_obj = dist(sdmat, diag = TRUE, upper = TRUE)
  dist_mat = as.matrix(dist_mat_obj)
  
  ## Telling the computer wich method to use to solve
  t_max = 60*5
  solver = S
  approximate = 0
  solver = list(name = solver, t_max = t_max, approximate = approximate, 
                round_cplex = 0, trace_cplex = 0)
  
  ## Solving
  out = nmatch(dist_mat = dist_mat, total_pairs = floor(N/2), 
               solver = solver, subset_weight = NULL)
  
  # These guys have the row numbers of the matched pairs
  id_1 = out$id_1  
  id_2 = out$id_2 
  
  ## If there are any leftovers they get assigned where
  ## the user wants them assigned
  X = dim(D)[1] %% 2 == 0
  if(X == FALSE) {LO = sum(1:dim(D)[1]) - sum(id_1, id_2)}
  
  ## Will be used in the loop below - a place to put Ks, 
  ## filled below, then graphed after using parallel coordinate plot
  Ks = matrix(NA, nrow = M, ncol = r.I)
  
  ## Used to run the loop below
  M.seq = seq_along(1:M)
  
  for(i in M.seq){
    ## Randomising once - 0 ctl, 1 trt, (subtract one because when 
    ## making Trt below the middle step has to identify the length of 
    ## Trt or it gets fussy and won't add on the last value)
    R = replicate(length(id_1), rbinom(1, size = 1, prob = 0.5))
    S = 1 - R
    
    ## The 1 is for the leftover row that goes to the trt arm
    Trt = c(R, S)
    
    ## Making the data, TA is included and goes to the trt arm
    if (X == TRUE) {
      Dt = data.frame(mymat[c(id_1, id_2),], Trt)
    } else {
      Dt = data.frame(mymat[c(id_1, id_2, LO),], c(Trt, (ToC - 1)))
    }
    
    ## Picking out trt and ctl covariates
    Trt.CV = Dt[which(Trt == 1), 1:r.I]
    Ctl.CV = Dt[-which(Trt == 1), 1:r.I]
    
    Ks[i,] = abs((apply(Trt.CV, 2, sum) - 
                    apply(Ctl.CV, 2, sum))/N)
  }## ending for i in M.seq
  
  ## So the labels in parcoord come out nicely
  ## NEW colnames here
  colnames(Ks) = I[,'L']
  
  P = list()
  P[[1]] = cbind(id_1, id_2)
  P[[2]] = I
  P[[3]] = as.data.frame(Ks)
  P[[4]] = Trt.CV
  P[[5]] = Ctl.CV
  
  P
} ## closing the function make.Ks

setwd("/Users/gwynn/Documents/PostDocKleinmanLab/Bins/Again1")
D = read_excel("SwapOut_Randomization_2017_4_27_FINAL.xlsx",  sheet=2)[1:137,]
D = as.data.frame(D)

setwd("/Users/gwynn/Documents/PostDocKleinmanLab/Bins/Shiny/Again/sage_latex_template_3")

## Final weighting for randomization
V = list(list("Attrib_Days_Month", 1, "Pt Days", 0, 80),
         list("Outcome_Saureus", 4, "S auer Rate", 0, 0.15),
         list("Outcome_MRSA", 2, "MRSA Rate", 0, 0.15),
         list("All_Blood", 4, "All Blood", 0, 0.15),
         list("Mup-R", 2, "Mup-R", 0, 0.02),
         list("Hx_MRSA", 1,"Hx MRSA", 0, 0.01),
         list("Mup_Adherence", 1, "Mup Adherence", 0, 0.05),
         list("CHG_Adherence", 1,"CHG Adherence", 0, 0.05),
         list("Median_LOS", 3,"Median LOS", 0, 0.05),
         list("Elix_Score", 1,"Comorbidity Score", 0, 0.15),
         list("Medicaid", 0,"Medicaid", 0, 0.02),
         list("DC_SNF", 0,"DC SNF", 0, 0.02),
         list("Surgery", 1,"Surgery", 0, 0.02),
         list("Onc_BMT_Trp", 2,"Onc_BMT_Trp", 0, 0.1),
         list("BMT_Trp", 0,"BMT_Trp", 0, 0.1),
         list("PCR_Blood", 0,"PCR_Blood", 0, 0.1)
         )

## First 3 weights zeroed
V1 = list(list("Attrib_Days_Month", 0, "Pt Days", 0, 80),
         list("Outcome_Saureus", 0, "S auer Rate", 0, 0.15),
         list("Outcome_MRSA", 0, "MRSA Rate", 0, 0.15),
         list("All_Blood", 4, "All Blood", 0, 0.15),
         list("Mup-R", 2, "Mup-R", 0, 0.02),
         list("Hx_MRSA", 1,"Hx MRSA", 0, 0.01),
         list("Mup_Adherence", 1, "Mup Adherence", 0, 0.05),
         list("CHG_Adherence", 1,"CHG Adherence", 0, 0.05),
         list("Median_LOS", 3,"Median LOS", 0, 0.05),
         list("Elix_Score", 1,"Comorbidity Score", 0, 0.15),
         list("Medicaid", 0,"Medicaid", 0, 0.02),
         list("DC_SNF", 0,"DC SNF", 0, 0.02),
         list("Surgery", 1,"Surgery", 0, 0.02),
         list("Onc_BMT_Trp", 2,"Onc_BMT_Trp", 0, 0.1),
         list("BMT_Trp", 0,"BMT_Trp", 0, 0.1),
         list("PCR_Blood", 0,"PCR_Blood", 0, 0.1)
         )

## Primary outcome weighted highly others zeroed
V2 = list(list("Attrib_Days_Month", 0, "Pt Days", 0, 80),
         list("Outcome_Saureus", 8, "S auer Rate", 0, 0.15),
         list("Outcome_MRSA", 0, "MRSA Rate", 0, 0.15),
         list("All_Blood", 4, "All Blood", 0, 0.15),
         list("Mup-R", 2, "Mup-R", 0, 0.02),
         list("Hx_MRSA", 1,"Hx MRSA", 0, 0.01),
         list("Mup_Adherence", 1, "Mup Adherence", 0, 0.05),
         list("CHG_Adherence", 1,"CHG Adherence", 0, 0.05),
         list("Median_LOS", 3,"Median LOS", 0, 0.05),
         list("Elix_Score", 1,"Comorbidity Score", 0, 0.15),
         list("Medicaid", 0,"Medicaid", 0, 0.02),
         list("DC_SNF", 0,"DC SNF", 0, 0.02),
         list("Surgery", 1,"Surgery", 0, 0.02),
         list("Onc_BMT_Trp", 2,"Onc_BMT_Trp", 0, 0.1),
         list("BMT_Trp", 0,"BMT_Trp", 0, 0.1),
         list("PCR_Blood", 0,"PCR_Blood", 0, 0.1)
         )

## Making data 
## The leftover is sent to the treatment arm (ToC = 2)
P = make.Ks(M = 300, D = D, Plot = "noscreen", vars = V, S = "glpk", ToC = 2)
P1 = make.Ks(M = 300, D = D, Plot = "noscreen", vars = V1, S = "glpk", ToC = 2)
P2 = make.Ks(M = 300, D = D, Plot = "noscreen", vars = V2, S = "glpk", ToC = 2)

## Making the things to plot - cols 1, 5 only
## Half the screen on bottom
A = makePlot(V = V, P = P, D = D, Tm = 300, NVs = 2, cols = 1:3)

## Top Left
A1 = makePlot(V = V1, P = P1, D = D, Tm = 300, NVs = 2, cols = 1:3)

## Top Right
A2 = makePlot(V = V2, P = P2, D = D, Tm = 300, NVs = 2, cols = 1:3)

## The bottom one
grid.newpage()

pushViewport(viewport(x=0, y=0, width=1, height=.5,
                      just=c("left", "bottom")))
drawDetails.lattice(A[[2]])
upViewport()

## Top Left
pushViewport(viewport(x=0, y=0.5, width=.52, height=.5,
                      just=c("left", "bottom")))
drawDetails.lattice(A1[[2]])
upViewport()

## Top Right
pushViewport(viewport(x=0.52, y=0.50, width=.48, height=.5,
                      just=c("left", "bottom")))
drawDetails.lattice(A2[[2]])
upViewport()

```

Prior to randomization, investigators spent time using a web application built using the \texttt{Shiny} package in \texttt{R}. The purpose of this is to determine which weights give advantageous balance across relevant baseline variables. 

## Discussion

Look how smart we are.

Use of color to make it clear where the closer to 0 ones are. Click on axis.


## Appendix
A more formal explanation of the variables here, to be checked with Susan.

